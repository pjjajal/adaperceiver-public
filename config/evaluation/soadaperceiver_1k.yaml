defaults:
  - override hydra/job_logging: none

seed: 1348

evaluation:
  output_csv: full_test_soada_results_token+depth+matdim_bs512.csv
  eval_depth: 21
  token_grans: [32, 64, 96, 128, 224, 256]
  mat_dims: [832] # 832 is the final

model:
  compile: true
  compile_mode: "default" # [default, max-autotune, no-autotune]
  model_checkpoint: null
  mat_dims: [416, 624, 832] # 832 is the final dimension.
  token_grans: [32, 64, 96, 128, 192, 256, 512]
  mask_type: block # [block, causal, null]
  encoder_config:
    # adapter type: [patch_embed, conv]
    adapter_type: "patch_embed"
    conv_adapter_model: null
    # image settings
    img_size: 224
    in_channels: 3
    use_fourier: false
    fourier_bands: 8
    # classification
    num_classes: 1000
    # encoder settings
    patch_size: 16
    num_heads: 13
    embed_dim: 832
    depth: 21
    max_latent_tokens: 256
    max_latent_tokens_mult: 2
    rope_theta: 10000
    ffn_ratio: 2.57
    qkv_bias: true
    proj_bias: true
    ls_init_values: 1.0e-6
    # layers
    act_layer: "gelu"
    norm_layer: "layernorm"
    ffn_layer: "mlp"
    attn_layer: "full"
    # other
    process_token_init: "learned"
    use_embed_ffn: true
    use_output_ffn: false
    # regularization
    proj_drop: 0.0
    attn_drop: 0.0
    drop_path: 0.0
    head_drop: 0.0

dataset:
  num_proc: 16
  name: imagenet
  num_classes: 1000
  # num_classes: 19167
  data_path: null
  transforms:
    image_size: 224
    val_image_size: 224
    max_crop_size: 256
    # RandAug
    rand_augment: true
    num_ops: 2
    magnitude: 20
    # normalize
    mean: [0.48145466, 0.4578275, 0.40821073]
    std: [0.26862954, 0.26130258, 0.27577711]
    # mixup, cutmix, random-erasing
    mixup: true
    mixup_alpha: 0.8
    cutmix: true
    cutmix_alpha: 0.5
    random_erasing: true
    erase_prob: 0.25


  # dataloader settings
  batch_size: 64
  num_workers: 8
  val_batch_size: 512
  val_num_workers: 8
  pin_memory: true

  # distributed sampler
  use_distributed_sampler: false # this is false because we are using hf datasets.

